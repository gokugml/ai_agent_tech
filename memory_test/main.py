#!/usr/bin/env python3
"""
AIå›å¤æ•ˆæœçœŸå®æµ‹è¯•æ¡†æ¶ - ä¸»å…¥å£

ä½¿ç”¨ç¤ºä¾‹:
    python main.py --scenario first_divination_basic --framework memobase --count 5
    python main.py --compare-frameworks --scenario-count 3
"""

import asyncio
import argparse
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

from loguru import logger
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, TaskID
from rich.panel import Panel
from rich.text import Text

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stderr, 
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}",
    level="INFO"
)

console = Console()

# å¯¼å…¥æ¡†æ¶æ¨¡å—
try:
    from memory_test.input_generation.template_designer import InputTemplateDesigner
    from memory_test.input_generation.ai_input_generator import AIInputGenerator
    from memory_test.response_testing.real_ai_tester import RealAITester
    from memory_test.response_testing.response_evaluator import ResponseQualityEvaluator
    from memory_test.framework_tests.memobase_real_test import MemobaseRealTester
    from memory_test.framework_tests.memu_real_test import MemuRealTester
    from memory_test.evaluation.conversation_analyzer import ConversationAnalyzer
    from memory_test.evaluation.memory_impact_assessor import MemoryImpactAssessor
    from memory_test.config import settings, ensure_results_directory
except ImportError as e:
    console.print(f"[red]å¯¼å…¥é”™è¯¯: {e}[/red]")
    console.print("[yellow]è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œï¼Œå¹¶ä¸”å·²å®‰è£…æ‰€éœ€ä¾èµ–[/yellow]")
    sys.exit(1)


class MemoryTestRunner:
    """è®°å¿†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.console = console
        self.results_dir = ensure_results_directory()
        
    async def run_single_framework_test(self,
                                      framework_name: str,
                                      scenario_id: str,
                                      input_count: int = 5) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ¡†æ¶æµ‹è¯•"""
        
        self.console.print(Panel(
            f"[bold blue]å¼€å§‹æµ‹è¯•æ¡†æ¶: {framework_name}[/bold blue]\\n"
            f"åœºæ™¯: {scenario_id}\\n"
            f"è¾“å…¥æ•°é‡: {input_count}",
            title="æµ‹è¯•å¼€å§‹"
        ))
        
        results = {
            "framework": framework_name,
            "scenario": scenario_id,
            "input_count": input_count,
            "start_time": datetime.now().isoformat(),
            "test_results": {}
        }
        
        try:
            with Progress() as progress:
                # æ­¥éª¤1: åˆ›å»ºè¾“å…¥æ¨¡æ¿
                task1 = progress.add_task("[cyan]åˆ›å»ºè¾“å…¥æ¨¡æ¿...", total=1)
                
                designer = InputTemplateDesigner()
                template = designer.create_personalized_template(scenario_id)
                
                progress.update(task1, completed=1)
                self.console.print("âœ… è¾“å…¥æ¨¡æ¿åˆ›å»ºå®Œæˆ")
                
                # æ­¥éª¤2: ç”ŸæˆAIè¾“å…¥
                task2 = progress.add_task("[cyan]ç”ŸæˆAIè¾“å…¥...", total=1)
                
                generator = AIInputGenerator()
                inputs = await generator.generate_user_input(template, input_count)
                
                progress.update(task2, completed=1)
                self.console.print(f"âœ… ç”Ÿæˆ {len(inputs)} ä¸ªAIè¾“å…¥")
                
                # æ­¥éª¤3: æµ‹è¯•AIå›å¤
                task3 = progress.add_task("[cyan]æµ‹è¯•AIå›å¤...", total=len(inputs))
                
                ai_tester = RealAITester(framework_name)
                session_id = ai_tester.create_test_session(template["user_context"])
                
                responses = []
                for i, input_data in enumerate(inputs):
                    # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡ - ä¿®å¤è®°å¿†é›†æˆé—®é¢˜
                    memory_context = self._build_memory_context(session_id, input_data, responses)
                    
                    response = await ai_tester.generate_ai_response(
                        session_id,
                        input_data.user_message,
                        memory_context
                    )
                    responses.append(response)
                    progress.update(task3, advance=1)
                
                self.console.print(f"âœ… å®Œæˆ {len(responses)} è½®AIå¯¹è¯")
                
                # æ­¥éª¤4: è¯„ä¼°å›å¤è´¨é‡
                task4 = progress.add_task("[cyan]è¯„ä¼°å›å¤è´¨é‡...", total=1)
                
                evaluator = ResponseQualityEvaluator()
                conversation_eval = evaluator.evaluate_conversation(responses)
                
                progress.update(task4, completed=1)
                self.console.print("âœ… å›å¤è´¨é‡è¯„ä¼°å®Œæˆ")
                
                # æ­¥éª¤5: æ·±åº¦å¯¹è¯åˆ†æ
                task5 = progress.add_task("[cyan]è¿›è¡Œæ·±åº¦åˆ†æ...", total=1)
                
                analyzer = ConversationAnalyzer()
                conversation_analysis = analyzer.analyze_single_conversation(responses)
                
                progress.update(task5, completed=1)
                self.console.print("âœ… æ·±åº¦å¯¹è¯åˆ†æå®Œæˆ")
            
            # æ•´ç†ç»“æœ
            results["test_results"] = {
                "responses": [
                    {
                        "user_input": r.user_input,
                        "ai_response": r.ai_response,
                        "response_time": r.response_time,
                        "timestamp": r.timestamp
                    } for r in responses
                ],
                "quality_evaluation": {
                    "overall_score": conversation_eval.overall_conversation_score,
                    "conversation_flow_score": conversation_eval.conversation_flow_score,
                    "memory_utilization_score": conversation_eval.memory_utilization_score,
                    "user_satisfaction_estimate": conversation_eval.user_satisfaction_estimate
                },
                "conversation_analysis": conversation_analysis,
                "session_summary": ai_tester.get_session_summary(session_id)
            }
            
            results["end_time"] = datetime.now().isoformat()
            results["success"] = True
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            self._display_test_summary(results)
            
            return results
            
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            results["error"] = str(e)
            results["success"] = False
            return results
    
    async def run_framework_comparison(self,
                                     scenario_ids: List[str],
                                     input_count_per_scenario: int = 3) -> Dict[str, Any]:
        """è¿è¡Œæ¡†æ¶å¯¹æ¯”æµ‹è¯•"""
        
        self.console.print(Panel(
            f"[bold magenta]å¼€å§‹æ¡†æ¶å¯¹æ¯”æµ‹è¯•[/bold magenta]\\n"
            f"æµ‹è¯•åœºæ™¯: {', '.join(scenario_ids)}\\n"
            f"æ¯åœºæ™¯è¾“å…¥æ•°: {input_count_per_scenario}",
            title="æ¡†æ¶å¯¹æ¯”æµ‹è¯•"
        ))
        
        comparison_results = {
            "test_type": "framework_comparison",
            "scenarios": scenario_ids,
            "input_count_per_scenario": input_count_per_scenario,
            "start_time": datetime.now().isoformat(),
            "frameworks": {}
        }
        
        frameworks = ["memobase", "memu"]
        all_responses = {"memobase": [], "memu": []}
        
        try:
            with Progress() as progress:
                total_tests = len(frameworks) * len(scenario_ids)
                main_task = progress.add_task("[bold green]æ€»ä½“è¿›åº¦", total=total_tests)
                
                for framework in frameworks:
                    framework_responses = []
                    
                    for scenario_id in scenario_ids:
                        self.console.print(f"\\n[bold cyan]æµ‹è¯• {framework.upper()} - {scenario_id}[/bold cyan]")
                        
                        # è¿è¡Œå•ä¸ªæµ‹è¯•
                        result = await self.run_single_framework_test(
                            framework, scenario_id, input_count_per_scenario
                        )
                        
                        if result["success"]:
                            # æå–å“åº”æ•°æ®ç”¨äºå¯¹æ¯”
                            responses_data = result["test_results"]["responses"]
                            framework_responses.extend(responses_data)
                        
                        progress.update(main_task, advance=1)
                    
                    all_responses[framework] = framework_responses
                    comparison_results["frameworks"][framework] = {
                        "total_responses": len(framework_responses),
                        "test_scenarios": scenario_ids
                    }
                
                # è¿›è¡Œæ¡†æ¶å¯¹æ¯”åˆ†æ
                if all_responses["memobase"] and all_responses["memu"]:
                    self.console.print("\\n[bold yellow]æ­£åœ¨è¿›è¡Œæ¡†æ¶å¯¹æ¯”åˆ†æ...[/bold yellow]")
                    
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨ä¸­éœ€è¦è½¬æ¢ä¸ºAIResponseå¯¹è±¡
                    comparison_summary = self._simple_framework_comparison(all_responses)
                    comparison_results["comparison_analysis"] = comparison_summary
                else:
                    self.console.print("\\n[red]æ¡†æ¶å¯¹æ¯”æ•°æ®ä¸è¶³ï¼Œè·³è¿‡å¯¹æ¯”åˆ†æ[/red]")
            
            comparison_results["end_time"] = datetime.now().isoformat()
            comparison_results["success"] = True
            
            # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
            self._display_comparison_results(comparison_results)
            
            return comparison_results
            
        except Exception as e:
            logger.error(f"æ¡†æ¶å¯¹æ¯”æµ‹è¯•å‡ºç°é”™è¯¯: {e}")
            comparison_results["error"] = str(e)
            comparison_results["success"] = False
            return comparison_results
    
    def _simple_framework_comparison(self, all_responses: Dict[str, List]) -> Dict[str, Any]:
        """ç®€å•çš„æ¡†æ¶å¯¹æ¯”åˆ†æ"""
        
        memobase_responses = all_responses["memobase"]
        memu_responses = all_responses["memu"]
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        memobase_avg_length = sum(len(r["ai_response"]) for r in memobase_responses) / len(memobase_responses)
        memu_avg_length = sum(len(r["ai_response"]) for r in memu_responses) / len(memu_responses)
        
        memobase_avg_time = sum(r["response_time"] for r in memobase_responses) / len(memobase_responses)
        memu_avg_time = sum(r["response_time"] for r in memu_responses) / len(memu_responses)
        
        # ç®€å•è¯„åˆ†ï¼ˆå®é™…ä½¿ç”¨ä¸­ä¼šæ›´å¤æ‚ï¼‰
        length_winner = "Memobase" if memobase_avg_length > memu_avg_length else "MemU"
        speed_winner = "Memobase" if memobase_avg_time < memu_avg_time else "MemU"
        
        return {
            "metrics": {
                "memobase": {
                    "avg_response_length": memobase_avg_length,
                    "avg_response_time": memobase_avg_time,
                    "total_responses": len(memobase_responses)
                },
                "memu": {
                    "avg_response_length": memu_avg_length, 
                    "avg_response_time": memu_avg_time,
                    "total_responses": len(memu_responses)
                }
            },
            "winners": {
                "content_richness": length_winner,
                "response_speed": speed_winner
            },
            "summary": f"å†…å®¹ä¸°å¯Œåº¦: {length_winner}, å“åº”é€Ÿåº¦: {speed_winner}"
        }
    
    def _display_test_summary(self, results: Dict[str, Any]) -> None:
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦"""
        
        if not results["success"]:
            self.console.print(f"[red]æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}[/red]")
            return
        
        test_results = results["test_results"]
        quality_eval = test_results["quality_evaluation"]
        
        # åˆ›å»ºç»“æœè¡¨æ ¼
        table = Table(title=f"æµ‹è¯•ç»“æœæ‘˜è¦ - {results['framework'].upper()}")
        table.add_column("æŒ‡æ ‡", style="cyan")
        table.add_column("æ•°å€¼", style="magenta")
        table.add_column("è¯„çº§", style="green")
        
        # æ·»åŠ æ•°æ®è¡Œ
        overall_score = quality_eval["overall_score"]
        table.add_row(
            "æ€»ä½“è´¨é‡åˆ†æ•°",
            f"{overall_score:.3f}",
            self._get_score_rating(overall_score)
        )
        
        flow_score = quality_eval["conversation_flow_score"] 
        table.add_row(
            "å¯¹è¯æµç•…åº¦",
            f"{flow_score:.3f}",
            self._get_score_rating(flow_score)
        )
        
        memory_score = quality_eval["memory_utilization_score"]
        table.add_row(
            "è®°å¿†åˆ©ç”¨åº¦",
            f"{memory_score:.3f}",
            self._get_score_rating(memory_score)
        )
        
        satisfaction = quality_eval["user_satisfaction_estimate"]
        table.add_row(
            "ç”¨æˆ·æ»¡æ„åº¦ä¼°ç®—",
            f"{satisfaction:.3f}",
            self._get_score_rating(satisfaction)
        )
        
        response_count = len(test_results["responses"])
        table.add_row("å¯¹è¯è½®æ•°", str(response_count), "ğŸ“Š")
        
        self.console.print(table)
        
        # æ˜¾ç¤ºå…³é”®æ´å¯Ÿ
        analysis = test_results["conversation_analysis"]
        insights = analysis.get("insights_and_recommendations", [])
        
        if insights:
            self.console.print("\\n[bold blue]å…³é”®æ´å¯Ÿ:[/bold blue]")
            for i, insight in enumerate(insights[:3], 1):
                self.console.print(f"  {i}. {insight}")
    
    def _display_comparison_results(self, results: Dict[str, Any]) -> None:
        """æ˜¾ç¤ºæ¡†æ¶å¯¹æ¯”ç»“æœ"""
        
        if not results["success"]:
            self.console.print(f"[red]å¯¹æ¯”æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}[/red]")
            return
        
        if "comparison_analysis" not in results:
            self.console.print("[yellow]ç¼ºå°‘å¯¹æ¯”åˆ†ææ•°æ®[/yellow]")
            return
        
        analysis = results["comparison_analysis"]
        
        # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
        table = Table(title="æ¡†æ¶å¯¹æ¯”ç»“æœ")
        table.add_column("æ¡†æ¶", style="cyan")
        table.add_column("å¹³å‡å›å¤é•¿åº¦", style="magenta")
        table.add_column("å¹³å‡å“åº”æ—¶é—´", style="green")
        table.add_column("æ€»å›å¤æ•°", style="yellow")
        
        metrics = analysis["metrics"]
        for framework, data in metrics.items():
            table.add_row(
                framework.upper(),
                f"{data['avg_response_length']:.0f} å­—ç¬¦",
                f"{data['avg_response_time']:.2f} ç§’",
                str(data['total_responses'])
            )
        
        self.console.print(table)
        
        # æ˜¾ç¤ºè·èƒœè€…
        winners = analysis["winners"]
        self.console.print(f"\\n[bold green]å¯¹æ¯”ç»“æœ:[/bold green]")
        self.console.print(f"â€¢ å†…å®¹ä¸°å¯Œåº¦è·èƒœ: [bold]{winners['content_richness']}[/bold]")
        self.console.print(f"â€¢ å“åº”é€Ÿåº¦è·èƒœ: [bold]{winners['response_speed']}[/bold]")
        
        self.console.print(f"\\n[italic]{analysis['summary']}[/italic]")
    
    def _get_score_rating(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è¯„çº§"""
        if score >= 0.9:
            return "ğŸŒŸ ä¼˜ç§€"
        elif score >= 0.8:
            return "âœ… è‰¯å¥½"
        elif score >= 0.7:
            return "ğŸ”¶ ä¸€èˆ¬"
        elif score >= 0.6:
            return "âš ï¸ å¾…æ”¹è¿›"
        else:
            return "âŒ è¾ƒå·®"
    
    def _build_memory_context(self, session_id: str, input_data, previous_responses: List) -> Dict[str, Any]:
        """æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡"""
        memory_context = {}
        
        # å¦‚æœæœ‰å†å²å¯¹è¯ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if previous_responses:
            # æå–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = []
            for response in previous_responses[-3:]:  # æœ€è¿‘3è½®å¯¹è¯
                recent_history.append({
                    "user": response.user_input,
                    "assistant": response.ai_response[:200] + "..." if len(response.ai_response) > 200 else response.ai_response
                })
            
            memory_context["conversation_history"] = recent_history
            
            # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„ç”¨æˆ·åå¥½å’Œå†å²ä¿¡æ¯
            memory_context["user_preferences"] = {
                "prefers_detailed_analysis": len(previous_responses) > 1,
                "communication_style": "direct" if len(input_data.user_message) < 20 else "detailed",
                "areas_of_interest": ["äº‹ä¸šå‘å±•", "äººé™…å…³ç³»", "è´¢è¿åˆ†æ"]
            }
            
            # æ¨¡æ‹Ÿä¸€äº›å†å²é¢„æµ‹è®°å½•
            if len(previous_responses) >= 2:
                memory_context["previous_predictions"] = {
                    "last_consultation_date": "2025-08-01",
                    "key_predictions": [
                        "äº‹ä¸šæ–¹é¢ä¼šæœ‰æ–°çš„æœºé‡",
                        "äººé™…å…³ç³»å°†æœ‰æ‰€æ”¹å–„",
                        "éœ€è¦æ³¨æ„å¥åº·æ–¹é¢çš„è°ƒå…»"
                    ],
                    "accuracy_feedback": "éƒ¨åˆ†é¢„æµ‹å·²ç»éªŒè¯"
                }
        
        return memory_context
    
    def _json_serializer(self, obj):
        """è‡ªå®šä¹‰JSONåºåˆ—åŒ–å™¨ï¼Œå¤„ç†dataclasså’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–å¯¹è±¡"""
        if hasattr(obj, '__dict__'):
            # å¯¹äºæœ‰__dict__å±æ€§çš„å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
            return obj.__dict__
        elif hasattr(obj, '_asdict'):
            # å¯¹äºnamedtuple
            return obj._asdict()
        elif str(type(obj)).startswith('<enum'):
            # å¯¹äºæšä¸¾ç±»å‹
            return obj.value
        else:
            # å¯¹äºå…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return str(obj)
    
    async def save_results(self, results: Dict[str, Any], filename_prefix: str = "test_results") -> str:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        filepath = self.results_dir / filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=self._json_serializer)
            
            self.console.print(f"\\n[green]æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}[/green]")
            return str(filepath)
            
        except Exception as e:
            self.console.print(f"\\n[red]ä¿å­˜ç»“æœå¤±è´¥: {e}[/red]")
            return ""


async def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(
        description="AIå›å¤æ•ˆæœçœŸå®æµ‹è¯•æ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py --scenario first_divination_basic --framework memobase --count 5
  python main.py --compare-frameworks --scenario-count 3
  python main.py --list-scenarios
        """
    )
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--scenario", type=str, default="first_divination_basic",
                       help="æµ‹è¯•åœºæ™¯ID (é»˜è®¤: first_divination_basic)")
    parser.add_argument("--framework", type=str, choices=["memobase", "memu"], 
                       default="memobase", help="è®°å¿†æ¡†æ¶ç±»å‹ (é»˜è®¤: memobase)")
    parser.add_argument("--count", type=int, default=5, 
                       help="ç”Ÿæˆè¾“å…¥çš„æ•°é‡ (é»˜è®¤: 5)")
    
    # å¯¹æ¯”æµ‹è¯•å‚æ•°
    parser.add_argument("--compare-frameworks", action="store_true",
                       help="è¿è¡Œæ¡†æ¶å¯¹æ¯”æµ‹è¯•")
    parser.add_argument("--scenario-count", type=int, default=3,
                       help="å¯¹æ¯”æµ‹è¯•ä¸­æ¯ä¸ªåœºæ™¯çš„è¾“å…¥æ•°é‡ (é»˜è®¤: 3)")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--list-scenarios", action="store_true", 
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•åœºæ™¯")
    parser.add_argument("--output-dir", type=str, 
                       help="ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: test_results)")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="è¯¦ç»†è¾“å‡ºæ¨¡å¼")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = MemoryTestRunner()
    
    try:
        # åˆ—å‡ºåœºæ™¯
        if args.list_scenarios:
            from input_generation.scenario_templates import ScenarioTemplateLibrary
            
            library = ScenarioTemplateLibrary()
            scenarios = library.get_all_template_ids()
            
            console.print("[bold blue]å¯ç”¨çš„æµ‹è¯•åœºæ™¯:[/bold blue]")
            for i, scenario in enumerate(scenarios, 1):
                template = library.get_template(scenario)
                console.print(f"  {i:2d}. [cyan]{scenario}[/cyan]")
                console.print(f"      {template.context_setup[:80]}...")
            return
        
        # è¿è¡Œæ¡†æ¶å¯¹æ¯”æµ‹è¯•
        if args.compare_frameworks:
            # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§åœºæ™¯
            scenarios = [
                "first_divination_basic",
                "skeptical_consultation", 
                "follow_up_session"
            ]
            
            results = await runner.run_framework_comparison(scenarios, args.scenario_count)
            await runner.save_results(results, "framework_comparison")
        
        # è¿è¡Œå•æ¡†æ¶æµ‹è¯•
        else:
            results = await runner.run_single_framework_test(
                args.framework, args.scenario, args.count
            )
            await runner.save_results(results, f"{args.framework}_test")
        
        console.print("\\n[bold green]æµ‹è¯•å®Œæˆ! ğŸ‰[/bold green]")
        
    except KeyboardInterrupt:
        console.print("\\n[yellow]æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
    except Exception as e:
        console.print(f"\\n[red]æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸçš„é”™è¯¯: {e}[/red]")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")


if __name__ == "__main__":
    asyncio.run(main())