import asyncio
from collections.abc import Sequence
from datetime import UTC, datetime
from typing import Annotated, Any, TypedDict

from langchain_anthropic import ChatAnthropic, convert_to_anthropic_tool
from langchain_core.messages import AIMessage, AnyMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

from auto_tools.tools import all_tools, sexual_selection_tool
from auto_tools.utils import get_memories_client
from user_config import settings

PROMPT = """ä½ æ˜¯ä¸€ä¸ªå…´è¶£çˆ±å¥½åˆ†æä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æµç¨‹åˆ†æèŠå¤©å†…å®¹ï¼š

1. é¦–å…ˆä½¿ç”¨ Sexual_Selection_Tool åˆ†æèŠå¤©å†…å®¹ä¸­è¯´è¯äººçš„æ€§åˆ«
2. æ ¹æ®è¯†åˆ«å‡ºçš„æ€§åˆ«ï¼Œè°ƒç”¨å¯¹åº”çš„å·¥å…·åˆ†æå…´è¶£çˆ±å¥½ï¼š
3. æ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼Œè¿”å›å®Œæ•´çš„å…´è¶£çˆ±å¥½æŠ¥å‘Š

è¯·ä¸¥æ ¼æŒ‰ç…§è¿™ä¸ªé¡ºåºæ‰§è¡Œï¼Œç¡®ä¿å…ˆåˆ†ææ€§åˆ«ï¼Œå†æ ¹æ®æ€§åˆ«è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚

IMPORTANT: å°½å¯èƒ½ä¸€æ¬¡æ€§è°ƒç”¨å¤šä¸ª tool
IMPORTANT: ä½¿ç”¨çº¯æ–‡æœ¬è¾“å‡ºï¼Œä¸è¦è§£é‡Šï¼Œç¦æ­¢è¾“å‡ºå¹»è§‰
"""


class State(TypedDict):
    messages: Annotated[Sequence[AnyMessage], add_messages]
    history_messages: Annotated[Sequence[AnyMessage], add_messages]


async def load_memories(state, config: RunnableConfig):
    """åŠ è½½å†å²æ¶ˆæ¯"""
    chat_message_history = get_memories_client(config["configurable"]["session_id"])  # type: ignore
    history_messages = chat_message_history.get_messages_with_count(200)
    return {"history_messages": history_messages}


def format_history(
    messages: Sequence[AnyMessage],
    tz: Any,
    human_prefix: str = "Human",
    ai_prefix: str = "AI",
) -> str:
    """æŠŠ BaseMessage åˆ—è¡¨æ ¼å¼åŒ–æˆå¸¦æ—¶é—´çš„å­—ç¬¦ä¸²ã€‚"""
    lines = []
    for msg in messages:
        ts: float | None = msg.additional_kwargs.get("chat_timestamp", None)
        if ts:
            local_ts = datetime.fromtimestamp(ts, tz=tz)
            ts_str = local_ts.isoformat()
        else:
            ts_str = "UNKNOWN"

        if isinstance(msg, HumanMessage):
            role = human_prefix
        elif isinstance(msg, AIMessage):
            role = ai_prefix
        else:
            raise ValueError("messages type error")
        lines.append(f"[{ts_str}] {role}: {msg.content}")
    return "\n".join(lines)


# åˆ›å»ºClaudeæ¨¡å‹å®ä¾‹
def create_claude_model():
    """åˆ›å»ºClaudeæ¨¡å‹ï¼Œæ”¯æŒç¼“å­˜"""
    api_key = settings.ANTHROPIC_API_KEY
    model = ChatAnthropic(
        model="claude-sonnet-4-20250514",
        api_key=api_key,
        base_url=settings.ANTHROPIC_BASE_URL,
        streaming=False,
        temperature=0.3,
        extra_headers={"anthropic-beta": "prompt-caching-2024-07-31"}
    )
    return model, PROMPT


model, system_prompt = create_claude_model()


async def agent(state: State):
    """Claudeä»£ç†èŠ‚ç‚¹"""
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=[
                    {
                        "type": "text",
                        "text": system_prompt
                    }
                ]
            ),
            HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": f"èŠå¤©å†…å®¹ï¼š{format_history(state['history_messages'], tz=UTC)}",
                        "cache_control": {"type": "ephemeral"}
                    }
                ]
            ),
            ("placeholder", "{messages}")
        ]
    )
    
    cache_sexual_selection_tool = convert_to_anthropic_tool(sexual_selection_tool)
    cache_sexual_selection_tool["cache_control"] = {"type": "ephemeral"}
    
    llm = prompt | model.bind_tools([*all_tools[:-1], cache_sexual_selection_tool])
    response = await llm.ainvoke({"messages": state["messages"]})
    
    # è¾“å‡ºå“åº”å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç¼“å­˜ç›¸å…³ä¿¡æ¯
    print("ğŸ“Š Claudeå“åº”å…ƒæ•°æ®:")
    print(response.response_metadata)
    print(response.usage_metadata["input_token_details"])
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜
    usage = response.response_metadata.get("usage", {})
    if "cache_creation_input_tokens" in usage or "cache_read_input_tokens" in usage:
        print("ğŸš€ æ£€æµ‹åˆ°ç¼“å­˜ä½¿ç”¨ï¼")
        if cache_creation := usage.get("cache_creation_input_tokens"):
            print(f"   ç¼“å­˜åˆ›å»ºTokenæ•°: {cache_creation}")
        if cache_read := usage.get("cache_read_input_tokens"):
            print(f"   ç¼“å­˜è¯»å–Tokenæ•°: {cache_read}")
    else:
        print("â„¹ï¸ æœªæ£€æµ‹åˆ°ç¼“å­˜ä½¿ç”¨")
    
    return {"messages": response}


# åˆ›å»ºLangGraphå·¥ä½œæµ
workflow = StateGraph(State)

workflow.add_node(load_memories)
workflow.add_node(agent)
workflow.add_node("tools", ToolNode(all_tools))

workflow.add_edge(START, "load_memories")
workflow.add_edge("load_memories", "agent")
workflow.add_conditional_edges(
    "agent",
    tools_condition,
    ["tools", END]
)
workflow.add_edge("tools", "agent")

graph = workflow.compile(name="claude_cache_test")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Claude Sonnet-4ç¼“å­˜æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥APIé…ç½®
    if not settings.ANTHROPIC_API_KEY:
        print("âŒ æœªé…ç½®ANTHROPIC_API_KEYï¼Œæ— æ³•è¿›è¡ŒClaudeæµ‹è¯•")
        return False
    
    print("âœ… Claude APIé…ç½®å·²å°±ç»ª")
    
    config = {
        "configurable":
            {
                "session_id": "ITFOPZQLOLVI"
            }
    }
    
    # æ‰§è¡Œç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆå¯èƒ½åˆ›å»ºç¼“å­˜ï¼‰
    print("\nğŸ“ æ‰§è¡Œç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼“å­˜åˆ›å»ºï¼‰...")
    start_time = asyncio.get_event_loop().time()
    
    async for event in graph.astream(
        input={}, 
        config=config, 
        debug=False
    ):
        print(f"äº‹ä»¶: {list(event.keys())}")
    
    first_call_time = asyncio.get_event_loop().time() - start_time
    print(f"â±ï¸ ç¬¬ä¸€æ¬¡è°ƒç”¨è€—æ—¶: {first_call_time:.2f}ç§’")
    
    # ç­‰å¾…ä¸€ç§’åæ‰§è¡Œç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
    await asyncio.sleep(1)
    print("\nğŸ“ æ‰§è¡Œç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜é‡ç”¨ï¼‰...")
    start_time = asyncio.get_event_loop().time()
    
    async for event in graph.astream(
        input={}, 
        config=config, 
        debug=False
    ):
        print(f"äº‹ä»¶: {list(event.keys())}")
    
    second_call_time = asyncio.get_event_loop().time() - start_time
    print(f"â±ï¸ ç¬¬äºŒæ¬¡è°ƒç”¨è€—æ—¶: {second_call_time:.2f}ç§’")
    
    # åˆ†ææ€§èƒ½å·®å¼‚
    if second_call_time < first_call_time * 0.8:
        print("ğŸ‰ æ£€æµ‹åˆ°æ˜æ˜¾çš„ç¼“å­˜æ€§èƒ½æå‡ï¼")
        print(f"æ€§èƒ½æå‡: {((first_call_time - second_call_time) / first_call_time * 100):.1f}%")
    else:
        print("â„¹ï¸ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ç¼“å­˜æ€§èƒ½æå‡")
    
    print("\nâœ… Claudeç¼“å­˜æµ‹è¯•å®Œæˆ")
    return True


if __name__ == "__main__":
    asyncio.run(main())