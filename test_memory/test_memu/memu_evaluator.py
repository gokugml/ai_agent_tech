"""
Memuè®°å¿†ç³»ç»Ÿèƒ½åŠ›è¯„æµ‹ä¸»ç¨‹åº

åŸºäºLangChainå’ŒMemuï¼Œå¯¹ä¸åŒåœºæ™¯ä¸‹çš„è®°å¿†èƒ½åŠ›è¿›è¡Œå®šé‡è¯„æµ‹
"""

import os
import sys
from typing import Dict, List, Any, Tuple
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '../../fortunetelling_memory_test'))

from memu import MemuClient
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

from test_memory.chats import chats
from test_memory.test_case import test_cases
from user_config import settings


class MemuEvaluator:
    """Memuè®°å¿†ç³»ç»Ÿè¯„æµ‹å™¨"""
    
    def __init__(self):
        # ä»fortunetelling_memory_testé¡¹ç›®è·å–Memué…ç½®
        try:
            from fortunetelling_memory_test.src.config import settings as ft_settings
            
            # åˆå§‹åŒ–Memuå®¢æˆ·ç«¯
            self.memu_client = MemuClient(
                base_url=ft_settings.MEMU_BASE_URL,
                api_key=ft_settings.MEMU_API_KEY
            )
            self.memu_settings = ft_settings
        except ImportError:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.memu_client = MemuClient(
                base_url="https://api.memu.so",
                api_key=os.getenv("MEMU_API_KEY")
            )
            self.memu_settings = None
        
        # åˆå§‹åŒ–Claudeæ¨¡å‹ç”¨äºè¯„åˆ†
        self.claude_model = ChatAnthropic(
            model="claude-3-5-sonnet-20241022",
            api_key=settings.ANTHROPIC_API_KEY.get_secret_value()
        )
        
        self.user_id = "test_memu_evaluation_001"
        self.agent_id = "test_chatbot"
        self.agent_name = "Test ChatBot"
        self.evaluation_results: List[Dict[str, Any]] = []
    
    def setup_test_data(self) -> None:
        """è®¾ç½®æµ‹è¯•æ•°æ®å¹¶å­˜å‚¨åˆ°Memu"""
        print("ğŸ”§ è®¾ç½®Memuæµ‹è¯•æ•°æ®...")
        
        try:
            # å°†æµ‹è¯•èŠå¤©æ•°æ®å­˜å‚¨åˆ°Memu
            conversation_pairs = []
            
            # å°†chatsè½¬æ¢ä¸ºå¯¹è¯å¯¹æ ¼å¼
            for i in range(0, len(chats), 2):
                if i + 1 < len(chats):
                    user_msg = chats[i]
                    assistant_msg = chats[i + 1]
                    
                    if user_msg["role"] == "user" and assistant_msg["role"] == "assistant":
                        conversation_text = f"user: {user_msg['content']}\n\nassistant: {assistant_msg['content']}"
                        conversation_pairs.append(conversation_text)
            
            print(f"ğŸ“ å‡†å¤‡å­˜å‚¨ {len(conversation_pairs)} å¯¹å¯¹è¯åˆ°Memu...")
            
            # å­˜å‚¨æ¯ä¸ªå¯¹è¯å¯¹åˆ°Memu
            for idx, conversation in enumerate(conversation_pairs):
                self.memu_client.memorize_conversation(
                    conversation=conversation,
                    user_id=self.user_id,
                    user_name="æµ‹è¯•ç”¨æˆ·",
                    agent_id=self.agent_id,
                    agent_name=self.agent_name,
                    session_date=datetime.now().isoformat(),
                )
                
                if idx % 5 == 0:  # æ¯5ä¸ªå¯¹è¯æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"  å·²å­˜å‚¨ {idx + 1}/{len(conversation_pairs)} å¯¹è¯")
            
            print(f"âœ… æˆåŠŸå­˜å‚¨ {len(conversation_pairs)} å¯¹å¯¹è¯åˆ°Memu")
            
        except Exception as e:
            print(f"âŒ æ•°æ®å­˜å‚¨å¤±è´¥: {e}")
            # ç»§ç»­è¯„æµ‹ï¼Œä½¿ç”¨ç©ºæ•°æ®
    
    def retrieve_clustered_categories(self, query: str) -> str:
        """æ‰§è¡Œèšç±»åˆ†ç±»æ£€ç´¢"""
        try:
            categories = self.memu_client.retrieve_related_clustered_categories(
                user_id=self.user_id,
                agent_id=self.agent_id,
                category_query=query
            )
            
            if not categories:
                return ""
            
            # æ ¼å¼åŒ–åˆ†ç±»ç»“æœ
            formatted_categories = []
            for category in categories:
                formatted_categories.append(
                    f"åˆ†ç±»: {getattr(category, 'category_name', 'unknown')}\n"
                    f"å†…å®¹: {getattr(category, 'content', str(category))}"
                )
            
            return "\n\n".join(formatted_categories)
            
        except Exception as e:
            print(f"âŒ èšç±»åˆ†ç±»æ£€ç´¢å¤±è´¥: {e}")
            return ""
    
    def retrieve_memory_items(self, query: str) -> str:
        """æ‰§è¡Œè®°å¿†é¡¹ç›®æ£€ç´¢"""
        try:
            memory_items = self.memu_client.retrieve_related_memory_items(
                query=query,
                user_id=self.user_id,
                agent_id=self.agent_id
            )
            
            if not memory_items:
                return ""
            
            # æ ¼å¼åŒ–è®°å¿†é¡¹ç›®ç»“æœ
            formatted_items = []
            for item in memory_items:
                formatted_items.append(
                    f"è®°å¿†ID: {getattr(item, 'id', 'unknown')}\n"
                    f"å†…å®¹: {getattr(item, 'content', str(item))}\n"
                    f"ç›¸å…³æ€§: {getattr(item, 'relevance_score', 'N/A')}"
                )
            
            return "\n\n".join(formatted_items)
            
        except Exception as e:
            print(f"âŒ è®°å¿†é¡¹ç›®æ£€ç´¢å¤±è´¥: {e}")
            return ""
    
    def evaluate_similarity(self, query: str, retrieved_content: str, expected_content: str, retrieval_type: str) -> Tuple[float, str]:
        """ä½¿ç”¨Claudeæ¨¡å‹è¯„ä¼°æ£€ç´¢å†…å®¹ä¸é¢„æœŸå†…å®¹çš„ç›¸ä¼¼åº¦"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®°å¿†ç³»ç»Ÿè¯„æµ‹ä¸“å®¶ã€‚è¯·è¯„ä¼°æ£€ç´¢åˆ°çš„å†…å®¹ä¸é¢„æœŸå†…å®¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

è¯„åˆ†æ ‡å‡† (0-10åˆ†):
- 10åˆ†ï¼šæ£€ç´¢å†…å®¹å®Œå…¨åŒ¹é…é¢„æœŸï¼ŒåŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ä¸”å‡†ç¡®
- 8-9åˆ†ï¼šæ£€ç´¢å†…å®¹åŸºæœ¬åŒ¹é…é¢„æœŸï¼ŒåŒ…å«å¤§éƒ¨åˆ†å…³é”®ä¿¡æ¯ï¼Œç»†èŠ‚ç•¥æœ‰å·®å¼‚  
- 6-7åˆ†ï¼šæ£€ç´¢å†…å®¹éƒ¨åˆ†åŒ¹é…é¢„æœŸï¼ŒåŒ…å«ä¸»è¦ä¿¡æ¯ï¼Œä½†ç¼ºå°‘ä¸€äº›é‡è¦ç»†èŠ‚
- 4-5åˆ†ï¼šæ£€ç´¢å†…å®¹ä¸é¢„æœŸæœ‰ä¸€å®šå…³è”ï¼Œä½†ä¿¡æ¯ä¸å®Œæ•´æˆ–æœ‰æ˜æ˜¾åå·®
- 2-3åˆ†ï¼šæ£€ç´¢å†…å®¹ä¸é¢„æœŸå…³è”åº¦è¾ƒä½ï¼Œå¤§éƒ¨åˆ†ä¿¡æ¯ä¸åŒ¹é…
- 0-1åˆ†ï¼šæ£€ç´¢å†…å®¹ä¸é¢„æœŸå®Œå…¨ä¸åŒ¹é…æˆ–ä¸ºç©º

è¯·æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦ç»™å‡º0-10çš„æ•°å­—è¯„åˆ†ï¼Œå¹¶ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±ã€‚"""

        human_prompt = f"""
ç”¨æˆ·æŸ¥è¯¢: {query}
æ£€ç´¢ç±»å‹: {retrieval_type}

æ£€ç´¢åˆ°çš„å†…å®¹:
{retrieved_content if retrieved_content else "ï¼ˆæ— æ£€ç´¢ç»“æœï¼‰"}

é¢„æœŸå†…å®¹:
{expected_content}

è¯·ç»™å‡ºè¯„åˆ† (0-10) å’Œç®€è¦ç†ç”±ï¼š"""

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = self.claude_model.invoke(messages)
            response_text = response.content
            
            # ä»å“åº”ä¸­æå–è¯„åˆ†
            score = self._extract_score_from_response(response_text)
            return score, response_text
            
        except Exception as e:
            print(f"âŒ AIè¯„åˆ†å¤±è´¥: {e}")
            return 0.0, f"è¯„åˆ†å¤±è´¥: {str(e)}"
    
    def _extract_score_from_response(self, response: str) -> float:
        """ä»Claudeå“åº”ä¸­æå–æ•°å­—è¯„åˆ†"""
        import re
        
        # å°è¯•æå–æ•°å­—è¯„åˆ†
        patterns = [
            r'è¯„åˆ†.*?([0-9](?:\.[0-9])?)',
            r'åˆ†æ•°.*?([0-9](?:\.[0-9])?)',
            r'([0-9](?:\.[0-9])?)\s*åˆ†',
            r'([0-9](?:\.[0-9])?)/10',
            r'([0-9](?:\.[0-9])?)\s*(?:åˆ†|$)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response)
            if matches:
                try:
                    score = float(matches[0])
                    return min(max(score, 0.0), 10.0)  # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
                except ValueError:
                    continue
        
        # å¦‚æœæ— æ³•æå–æ•°å­—ï¼Œè¿”å›é»˜è®¤å€¼
        return 0.0
    
    def evaluate_single_test_case(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„æµ‹å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        query = test_case["query"]
        expected = test_case["expected"]
        
        print(f"ğŸ” è¯„æµ‹æŸ¥è¯¢: {query}")
        
        # æ‰§è¡Œä¸¤ç§æ£€ç´¢
        clustered_result = self.retrieve_clustered_categories(query)
        memory_items_result = self.retrieve_memory_items(query)
        
        # AIè¯„åˆ†
        clustered_score, clustered_reason = self.evaluate_similarity(query, clustered_result, expected, "èšç±»åˆ†ç±»æ£€ç´¢")
        memory_items_score, memory_items_reason = self.evaluate_similarity(query, memory_items_result, expected, "è®°å¿†é¡¹ç›®æ£€ç´¢")
        
        result = {
            "query": query,
            "expected": expected,
            # æ£€ç´¢ç»“æœ
            "clustered_result": clustered_result,
            "memory_items_result": memory_items_result,
            # è¯„åˆ†ç»“æœ
            "clustered_score": clustered_score,
            "memory_items_score": memory_items_score,
            # è¯„åˆ†è¯¦æƒ…
            "clustered_evaluation": clustered_reason,
            "memory_items_evaluation": memory_items_reason,
            # ç»¼åˆè¯„åˆ†
            "overall_average": (clustered_score + memory_items_score) / 2,
            "core_method_score": memory_items_score  # memory_itemsä½œä¸ºæ ¸å¿ƒé€šç”¨æ–¹æ³•
        }
        
        print(f"  ğŸ“Š èšç±»åˆ†ç±»å¾—åˆ†: {clustered_score:.1f}/10")
        print(f"  ğŸ“Š è®°å¿†é¡¹ç›®å¾—åˆ†(é€šç”¨): {memory_items_score:.1f}/10")
        print(f"  ğŸ“Š ç»¼åˆå¹³å‡å¾—åˆ†: {result['overall_average']:.1f}/10\n")
        
        return result
    
    def evaluate_all_scenarios(self) -> Dict[str, Any]:
        """è¯„æµ‹æ‰€æœ‰è®°å¿†åœºæ™¯"""
        print("ğŸš€ å¼€å§‹Memuå®Œæ•´è®°å¿†èƒ½åŠ›è¯„æµ‹...\n")
        
        scenario_results = {}
        total_scores = {"clustered": [], "memory_items": [], "overall_average": [], "core_method": []}
        
        for scenario in test_cases:
            scenario_name = scenario["sence"]
            test_case_list = scenario["test_case"]
            
            print(f"ğŸ“‹ è¯„æµ‹åœºæ™¯: {scenario_name}")
            print("=" * 50)
            
            scenario_result = {
                "scenario_name": scenario_name,
                "test_results": [],
                "scenario_clustered_avg": 0,
                "scenario_memory_items_avg": 0,
                "scenario_overall_avg": 0
            }
            
            clustered_scores = []
            memory_items_scores = []
            
            for test_case in test_case_list:
                result = self.evaluate_single_test_case(test_case)
                scenario_result["test_results"].append(result)
                
                clustered_scores.append(result["clustered_score"])
                memory_items_scores.append(result["memory_items_score"])
                total_scores["clustered"].append(result["clustered_score"])
                total_scores["memory_items"].append(result["memory_items_score"])
                total_scores["overall_average"].append(result["overall_average"])
                total_scores["core_method"].append(result["core_method_score"])
            
            # è®¡ç®—åœºæ™¯å¹³å‡åˆ†
            scenario_result["scenario_clustered_avg"] = sum(clustered_scores) / len(clustered_scores)
            scenario_result["scenario_memory_items_avg"] = sum(memory_items_scores) / len(memory_items_scores)
            scenario_result["scenario_overall_avg"] = (scenario_result["scenario_clustered_avg"] + 
                                                     scenario_result["scenario_memory_items_avg"]) / 2
            
            scenario_results[scenario_name] = scenario_result
            
            print(f"ğŸ¯ åœºæ™¯ '{scenario_name}' è¯„æµ‹å®Œæˆ:")
            print(f"  èšç±»åˆ†ç±»å¹³å‡åˆ†: {scenario_result['scenario_clustered_avg']:.2f}/10")
            print(f"  è®°å¿†é¡¹ç›®å¹³å‡åˆ†: {scenario_result['scenario_memory_items_avg']:.2f}/10") 
            print(f"  åœºæ™¯æ€»åˆ†: {scenario_result['scenario_overall_avg']:.2f}/10\n")
        
        # è®¡ç®—æ•´ä½“è¯„æµ‹ç»“æœ
        overall_results = {
            "scenario_results": scenario_results,
            "overall_clustered_avg": sum(total_scores["clustered"]) / len(total_scores["clustered"]),
            "overall_memory_items_avg": sum(total_scores["memory_items"]) / len(total_scores["memory_items"]),
            "overall_average": sum(total_scores["overall_average"]) / len(total_scores["overall_average"]),
            "overall_core_method_avg": sum(total_scores["core_method"]) / len(total_scores["core_method"]),
            "total_test_cases": len(total_scores["overall_average"]),
            "evaluation_timestamp": datetime.now().isoformat(),
            "memory_framework": "Memu",
            "methods_tested": ["retrieve_related_clustered_categories", "retrieve_related_memory_items"]
        }
        
        return overall_results
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ§  Memuè®°å¿†ç³»ç»Ÿèƒ½åŠ›è¯„æµ‹æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"è¯„æµ‹æ—¶é—´: {results['evaluation_timestamp']}")
        report.append(f"è®°å¿†æ¡†æ¶: {results['memory_framework']}")
        report.append(f"æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {results['total_test_cases']}")
        report.append("")
        
        # æ•´ä½“è¯„æµ‹ç»“æœ
        report.append("ğŸ“Š æ•´ä½“è¯„æµ‹ç»“æœ")
        report.append("-" * 40)
        report.append(f"èšç±»åˆ†ç±»æ£€ç´¢å¹³å‡åˆ†: {results['overall_clustered_avg']:.2f}/10")
        report.append(f"è®°å¿†é¡¹ç›®æ£€ç´¢å¹³å‡åˆ†: {results['overall_memory_items_avg']:.2f}/10")
        report.append(f"ç»¼åˆå¹³å‡åˆ†: {results['overall_average']:.2f}/10")
        
        # è¯„çº§
        avg_score = results['overall_average']
        if avg_score >= 9.0:
            grade = "ğŸ† ä¼˜ç§€ (A+)"
        elif avg_score >= 8.0:
            grade = "ğŸ¥‡ è‰¯å¥½ (A)"
        elif avg_score >= 7.0:
            grade = "ğŸ¥ˆ ä¸­ç­‰ (B)"
        elif avg_score >= 6.0:
            grade = "ğŸ¥‰ åŠæ ¼ (C)"
        else:
            grade = "âŒ éœ€æ”¹è¿› (D)"
        
        report.append(f"æ•´ä½“è¯„çº§: {grade}")
        report.append("")
        
        # å„åœºæ™¯è¯¦ç»†ç»“æœ
        report.append("ğŸ“‹ å„åœºæ™¯è¯¦ç»†ç»“æœ")
        report.append("-" * 40)
        
        for scenario_name, scenario_result in results["scenario_results"].items():
            report.append(f"\nğŸ¯ {scenario_name}")
            report.append(f"  èšç±»åˆ†ç±»å¹³å‡: {scenario_result['scenario_clustered_avg']:.2f}/10")
            report.append(f"  è®°å¿†é¡¹ç›®å¹³å‡: {scenario_result['scenario_memory_items_avg']:.2f}/10")
            report.append(f"  åœºæ™¯æ€»åˆ†: {scenario_result['scenario_overall_avg']:.2f}/10")
            
            # æ˜¾ç¤ºè¯¥åœºæ™¯ä¸‹è¡¨ç°æœ€å¥½å’Œæœ€å·®çš„æµ‹è¯•ç”¨ä¾‹
            test_results = scenario_result["test_results"]
            if test_results:
                best_case = max(test_results, key=lambda x: x["average_score"])
                worst_case = min(test_results, key=lambda x: x["average_score"])
                
                report.append(f"  âœ… æœ€ä½³è¡¨ç° ({best_case['average_score']:.1f}åˆ†): {best_case['query']}")
                report.append(f"  âš ï¸ å¾…æ”¹è¿› ({worst_case['average_score']:.1f}åˆ†): {worst_case['query']}")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def save_detailed_results(self, results: Dict[str, Any], filename: str = "memu_evaluation_detailed.json") -> None:
        """ä¿å­˜è¯¦ç»†è¯„æµ‹ç»“æœåˆ°JSONæ–‡ä»¶"""
        output_path = os.path.join(os.path.dirname(__file__), filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ è¯¦ç»†è¯„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    
    def run_evaluation(self) -> None:
        """è¿è¡Œå®Œæ•´çš„è®°å¿†èƒ½åŠ›è¯„æµ‹"""
        print("ğŸ§  Memuè®°å¿†ç³»ç»Ÿèƒ½åŠ›è¯„æµ‹ç¨‹åºå¯åŠ¨")
        print("=" * 50)
        
        try:
            # 1. è®¾ç½®æµ‹è¯•ç¯å¢ƒ
            # self.setup_test_data()
            
            # 2. æ‰§è¡Œè¯„æµ‹
            results = self.evaluate_all_scenarios()
            
            # 3. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(results)
            print(report)
            
            # 4. ä¿å­˜è¯¦ç»†ç»“æœ
            self.save_detailed_results(results)
            
            print("âœ… Memuè®°å¿†èƒ½åŠ›è¯„æµ‹å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """ä¸»å‡½æ•°"""
    evaluator = MemuEvaluator()
    evaluator.run_evaluation()


if __name__ == "__main__":
    main()